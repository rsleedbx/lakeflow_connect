{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72289222-d527-4027-b33b-cf8128d7a933",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Instructions\n",
    "\n",
    "Based on https://docs.databricks.com/_extras/notebooks/source/sql-server-cdc-connector-setup.html \n",
    "* Modify settings in \"Configuration\" cell as necessary.\n",
    "* Execute the following cells one after another. Some are optional and can be skipped.\n",
    "\n",
    "# Prerequisite\n",
    "- cdc and ct enabled on the catalog\n",
    "- connection_name created\n",
    "- secrets has the user catalog credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "670e94fc-6cda-4c34-92b0-b2678372ad7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ensure recent version of the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9986908-aeb9-4975-9a6c-eb436fa96df6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# public example has ~=0.28.0.  this example does not require the version restriction\n",
    "%pip install --quiet databricks-sdk pymssql sqlalchemy jinja2 apscheduler\n",
    "try:\n",
    "    dbutils.library.restartPython()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67ab6dca-56a3-457b-8173-55f9c6c5a864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.service import catalog, jobs, pipelines\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "import logging\n",
    "\n",
    "w = WorkspaceClient()\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.start()\n",
    "logging.basicConfig()\n",
    "logging.getLogger('apscheduler').setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time, base64\n",
    "from datetime import datetime, timedelta\n",
    "WHOAMI=re.sub('[-.@]','', w.current_user.me().user_name.split('@')[0])   \n",
    "NINE_CHAR_ID=hex(int(time.time_ns() / 100000000 ))[2:]\n",
    "SECRETS_SCOPE=\"lfcddemo\"\n",
    "\n",
    "# convert secrets suffix to connection name suffix\n",
    "PREFERRED_HOST_SUFFIX={ \"-sq\":\"sq\", \"-gt\":\"gt\", }\n",
    "# get the secrets\n",
    "scrts=w.secrets.list_secrets(scope=SECRETS_SCOPE)\n",
    "# get preferred secrets key\n",
    "SECRETS_KEY=\"\"\n",
    "for host_suffix in PREFERRED_HOST_SUFFIX:\n",
    "    for scrt in scrts:\n",
    "        if scrt.key.endswith(host_suffix): \n",
    "            SECRETS_KEY=scrt.key\n",
    "            break\n",
    "    if SECRETS_KEY:\n",
    "        break    \n",
    "print(f\"using databricks secretes {SECRETS_SCOPE=} {scrt.key=}\")\n",
    "\n",
    "# convert export key=value to key=value dict\n",
    "scrt_key_values_str = base64.b64decode(w.secrets.get_secret(scope=SECRETS_SCOPE, key=SECRETS_KEY).value).decode('utf-8')\n",
    "scrt_key_value_dict={}\n",
    "for export_key_value in scrt_key_values_str.split(\";\"):\n",
    "    export_key_values=re.split(\"[ =']\",export_key_value)\n",
    "    if len(export_key_values) > 1:\n",
    "        scrt_key_value_dict[export_key_values[1]]=export_key_values[3]\n",
    "\n",
    "print(f\"retrieved databricks secretes {SECRETS_SCOPE=} {scrt.key=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e415f98e-e939-4278-b91a-565d24b0c7a9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ======================\n",
    "# Setup\n",
    "# ======================\n",
    "\n",
    "# The following function simplifies the replication of multiple tables from the same schema\n",
    "def replicate_tables_from_db_schema(db_catalog_name, db_schema_name, db_table_names):\n",
    "  return [pipelines.IngestionConfig(\n",
    "            table = pipelines.TableSpec( \n",
    "            source_catalog=db_catalog_name,\n",
    "            source_schema=db_schema_name,\n",
    "            source_table=table_name,\n",
    "            destination_catalog=target_catalog_name,\n",
    "            destination_schema=target_schema_name,\n",
    "          )) for table_name in db_table_names]\n",
    "\n",
    "# The following function simplifies the replication of an entire DB schemas\n",
    "def replicate_full_db_schema(db_catalog_name, db_schema_names):\n",
    "  return [pipelines.IngestionConfig(\n",
    "            schema = pipelines.SchemaSpec( \n",
    "            source_catalog=db_catalog_name,\n",
    "            source_schema=db_schema_name,\n",
    "            destination_catalog=target_catalog_name,\n",
    "            destination_schema=target_schema_name,\n",
    "          )) for db_schema_name in db_schema_names]\n",
    "\n",
    "\n",
    "mode=\"BOTH\" # CDC | CT | BOTH | NONE\n",
    "gateway_cluster_spec = None\n",
    "# Uncomment the following to specify a cluster policy and/or spark configuration for the Gateway pipeline\n",
    "# gateway_cluster_spec = pipelines.PipelineCluster(\n",
    "#   # Uncomment to specifya a cluster policy  \n",
    "#   # label=\"default\", \n",
    "#   # policy_id=\"0011223344556677\", \n",
    "#   # apply_policy_default_values=True,\n",
    "#\n",
    "#   # Uncomment to customize cluser Spark configuration\n",
    "#   spark_conf={ }\n",
    "#)\n",
    "\n",
    "# The name of the UC connection with the credentials to access the source database\n",
    "connection_name = f\"lfcddemo-{PREFERRED_HOST_SUFFIX[host_suffix]}\"\n",
    "\n",
    "# The name of the UC catalog and schema to store the replicated tables\n",
    "target_catalog_name = \"main\"\n",
    "target_schema_name = f\"{WHOAMI}_{NINE_CHAR_ID}\"\n",
    "\n",
    "# The name of the UC catalog and schema to store the staging volume with intermediate\n",
    "# CDC and snapshot data.\n",
    "# Use the destination catalog/schema by default\n",
    "stg_catalog_name = target_catalog_name \n",
    "stg_schema_name = target_schema_name \n",
    "\n",
    "# The name of the Gateway pipeline to create\n",
    "gateway_pipeline_name = f\"{WHOAMI}_{NINE_CHAR_ID}_gw\"\n",
    "\n",
    "# The name of the Ingestion pipeline to create\n",
    "ingestion_pipeline_name = f\"{WHOAMI}_{NINE_CHAR_ID}_ig\"\n",
    "\n",
    "# source\n",
    "source_catalog_name=\"\"              # get from secrets \n",
    "source_schema_name=f\"{WHOAMI}\"\n",
    "\n",
    "# Customize who gets notified about failures\n",
    "notifications = [\n",
    "  pipelines.Notifications(\n",
    "      email_recipients = [ w.current_user.me().user_name ],\n",
    "      alerts = [ \"on-update-failure\", \"on-update-fatal-failure\", \"on-flow-failure\"]\n",
    "      )\n",
    "  ]\n",
    "\n",
    "# Get connections\n",
    "connections = w.connections.get(connection_name)\n",
    "\n",
    "print(f\"using databricks connections {connection_name=}\")\n",
    "\n",
    "if scrt_key_value_dict['DB_HOST_FQDN'] != connections.options['host']:\n",
    "    raise exception(f\"{connections.options['host']=} and {scrt_key_value_dict['DB_HOST_FQDN']=} do not match \")\n",
    "if scrt_key_value_dict['DB_PORT'] != connections.options['port']:\n",
    "    raise exception(f\"{connections.options['port']=} and {scrt_key_value_dict['DB_PORT']=} do not match \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create connection pool to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to source with id that can run CDC / CT command\n",
    "run_cdc_ct=True\n",
    "\n",
    "import sqlalchemy as sa, pandas as pd, getpass, jinja2\n",
    "source_host_name=connections.options[\"host\"]\n",
    "source_port=connections.options[\"port\"]\n",
    "\n",
    "if \"source_user_name\" not in vars() or not source_user_name: \n",
    "    try:\n",
    "        source_user_name=scrt_key_value_dict['USER_USERNAME']\n",
    "    except:\n",
    "        source_user_name=input(\"dba source_user_name\")\n",
    "if \"source_password\" not in vars() or not source_password: \n",
    "    try:\n",
    "        source_password=scrt_key_value_dict['USER_PASSWORD']\n",
    "    except:\n",
    "        source_password=getpass.getpass(\"dba source_password\")\n",
    "if \"source_catalog_name\" not in vars() or not source_catalog_name: \n",
    "    try:\n",
    "        source_catalog_name=scrt_key_value_dict['DB_CATALOG']\n",
    "    except:\n",
    "        source_catalog_name=getpass.getpass(\"source_catalog_name\")\n",
    "\n",
    "sqlalchemy_url=f\"mssql+pymssql://{source_user_name}:{source_password}@{source_host_name}:{source_port}/{source_catalog_name}\"\n",
    "engine = sa.create_engine(sqlalchemy_url, pool_size=20, max_overflow=0, pool_pre_ping=True, isolation_level=\"AUTOCOMMIT\", \n",
    "    connect_args={\"login_timeout\": 120})\n",
    "\n",
    "# azure SQL requires time to wake up after the first login attempt\n",
    "connect_retry=0\n",
    "conn=None\n",
    "while (not conn) or (connect_retry < 10):\n",
    "    try:    \n",
    "        conn = engine.connect()\n",
    "        break\n",
    "    except Exception as e:\n",
    "        connect_retry += 1\n",
    "        print(f\"Sleeping for retry.  Database error: {e}\")\n",
    "        time.sleep(60)\n",
    "if (not conn):\n",
    "    raise Exception(f\"Could not connect to {source_user_name}:@{source_host_name}:{source_port}/{source_catalog_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the full list of tables to replicate\n",
    "# IMPORTANT: The letter case of the catalog, schema and table names MUST MATCH EXACTLY the case used in the source database system tables\n",
    "tables_to_replicate = replicate_full_db_schema(source_catalog_name, [source_schema_name]) \n",
    "\n",
    "# Append tables from additional schemas as needed\n",
    "#  + replicate_tables_from_db_schema(source_catalog_name, source_schema_name, [\"table_name_1\", \"table_name_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Connect, Create two tables, populate with some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(sa.text(\"select 1\")).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_SCHEMA=f\"{WHOAMI}\"\n",
    "conn.execute(sa.text(f\"\"\"\n",
    "IF NOT EXISTS (SELECT 1 FROM sys.schemas WHERE name = '{DB_SCHEMA}')\n",
    "BEGIN\n",
    "    EXEC('CREATE SCHEMA [{DB_SCHEMA}]')\n",
    "    select 'created'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    select 'already exists'\n",
    "END\n",
    "\"\"\")).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(sa.text(f\"\"\"\n",
    "IF OBJECT_ID(N'[{DB_SCHEMA}].[intpk]', N'U') is NULL\n",
    "BEGIN\n",
    "    create table [{DB_SCHEMA}].[intpk] (pk int IDENTITY NOT NULL primary key, dt datetime)\n",
    "    select 'created'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    select 'exists'\n",
    "END\n",
    "\"\"\")).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(sa.text(f\"\"\"\n",
    "IF OBJECT_ID(N'[{DB_SCHEMA}].[dtix]', N'U') is NULL\n",
    "BEGIN\n",
    "    create table [{DB_SCHEMA}].[dtix] (dt datetime)\n",
    "    select 'created'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    select 'exists'\n",
    "END\n",
    "\"\"\")).fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable CD / CT on the select schema and select list of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(sa.text(f\"\"\"\n",
    "BEGIN\n",
    "DECLARE @ct_enabled_on_cat INT;\n",
    "DECLARE @cdc_enabled_on_cat INT;\n",
    "-- set if cdc or ct is enabled on the catalog\n",
    "\n",
    "if exists(select is_cdc_enabled from sys.databases where name=db_name() and is_cdc_enabled=1)\n",
    "    set @cdc_enabled_on_cat = 1;\n",
    "else\n",
    "    set @cdc_enabled_on_cat = 0;\n",
    "\n",
    "if exists(select database_id from sys.change_tracking_databases where database_id=db_id())\n",
    "    set @ct_enabled_on_cat = 1;\n",
    "else\n",
    "    set @ct_enabled_on_cat = 0;\n",
    "select @cdc_enabled_on_cat as cdc_enabled_on_cat, @ct_enabled_on_cat as ct_enabled_on_cat\n",
    "END\n",
    "\"\"\")).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tabulate tables per schema\n",
    "s_t_filter={}\n",
    "for s_or_t in tables_to_replicate:\n",
    "    if s_or_t.schema and s_or_t.schema.source_schema not in s_t_filter:  s_t_filter[s_or_t.schema.source_schema] = set()\n",
    "    else:\n",
    "        if s_or_t.table.source_schema not in s_t_filter: s_t_filter[s_or_t.table.source_schema] = set()\n",
    "        s_t_filter[s_or_t.table.source_schema].add(s_or_t.table.source_table)\n",
    "# build where clause\n",
    "s_t_filter_sql_list=[]\n",
    "for s_name, t_name in s_t_filter.items():\n",
    "    schema_filter = f\"table_schema='{s_name}'\" \n",
    "    table_filter = \",\".join(f\"'{table_name}'\" for table_name in t_name)\n",
    "    if table_filter: s_t_filter_sql_list.append(f\"({schema_filter} and table_name in ({table_filter}))\")\n",
    "    else: s_t_filter_sql_list.append(f\"({schema_filter})\") \n",
    "s_t_filter_sql = \"and (\" + \" or \".join(s_t_filter_sql_list) + \")\" if s_t_filter_sql_list else \"\"\n",
    "\n",
    "# tsql\n",
    "cdc_ct_enable_tsql=\"\"\"\n",
    "DECLARE @cdc_enabled_count INT = 0;\n",
    "DECLARE @cdc_disabled_count INT = 0;\n",
    "DECLARE @ct_enabled_count INT = 0;\n",
    "DECLARE @ct_disabled_count INT = 0;\n",
    "DECLARE @cdc_enabled_already_count INT = 0;\n",
    "DECLARE @cdc_disabled_already_count INT = 0;\n",
    "DECLARE @ct_enabled_already_count INT = 0;\n",
    "DECLARE @ct_disabled_already_count INT = 0;\n",
    "\n",
    "OPEN MyCursor\n",
    "FETCH NEXT FROM MyCursor INTO @TABLE_CAT, @TABLE_SCHEM, @TABLE_NAME, @PK, @CDC, @CT\n",
    "WHILE @@FETCH_STATUS = 0\n",
    "BEGIN\n",
    "    if (@PK is NULL) \n",
    "      if (@mode='CDC' or @mode='BOTH') and (@cdc_enabled_on_cat = 1) \n",
    "        -- need to enable CDC if not enabled\n",
    "        if @CDC is NULL \n",
    "        BEGIN\n",
    "            exec sys.sp_cdc_enable_table @source_schema = @TABLE_SCHEM, @source_name = @TABLE_NAME,  @role_name = NULL, @supports_net_changes = 0;\n",
    "            set @cdc_enabled_count = @cdc_enabled_count + 1;\n",
    "        END\n",
    "        else\n",
    "        BEGIN\n",
    "            set @cdc_enabled_already_count = @cdc_enabled_already_count + 1;\n",
    "        END\n",
    "      else \n",
    "        -- need to disable CDC if enabled\n",
    "        if @CDC is not NULL\n",
    "        BEGIN\n",
    "            exec sys.sp_cdc_disable_table @source_schema = @TABLE_SCHEM, @source_name = @TABLE_NAME,  @capture_instance = 'all';\n",
    "            set @cdc_disabled_count = @cdc_disabled_count + 1;\n",
    "        END\n",
    "        else\n",
    "        BEGIN\n",
    "            set @cdc_disabled_already_count = @cdc_disabled_already_count + 1;\n",
    "        END\n",
    "\n",
    "    if (@PK is NOT NULL)\n",
    "      if (@mode='CT' or @mode='BOTH') and (@ct_enabled_on_cat = 1) \n",
    "        -- need to enable CT if not enabled\n",
    "        if @CT is NULL \n",
    "        BEGIN\n",
    "            exec('ALTER TABLE ['+@TABLE_SCHEM+'].['+@TABLE_NAME+'] ENABLE CHANGE_TRACKING WITH (TRACK_COLUMNS_UPDATED = ON)');\n",
    "            set @ct_enabled_count = @ct_enabled_count + 1;\n",
    "        END\n",
    "        else\n",
    "        BEGIN\n",
    "            set @ct_enabled_already_count = @ct_enabled_already_count + 1;\n",
    "        END\n",
    "      else \n",
    "        -- need to disable CT if enabled\n",
    "        if @CT is not NULL\n",
    "        BEGIN\n",
    "            exec('ALTER TABLE ['+@TABLE_SCHEM+'].['+@TABLE_NAME+'] DISABLE CHANGE_TRACKING')\t\n",
    "            set @ct_disabled_count = @ct_disabled_count + 1;\n",
    "        END\n",
    "        else\n",
    "        BEGIN\n",
    "            set @ct_disabled_already_count = @ct_disabled_already_count + 1;\n",
    "        END\n",
    "    -- fetch next    \n",
    "    FETCH NEXT FROM MyCursor INTO @TABLE_CAT, @TABLE_SCHEM, @TABLE_NAME, @PK, @CDC, @CT;\n",
    "END\n",
    "CLOSE MyCursor;\n",
    "DEALLOCATE MyCursor;\n",
    "\n",
    "SELECT  \n",
    "    @cdc_enabled_count cdc_enabled_count,\n",
    "    @cdc_disabled_count cdc_disabled_count,\n",
    "    @ct_enabled_count ct_enabled_count,\n",
    "    @ct_disabled_count ct_disabled_count,\n",
    "    @cdc_enabled_already_count cdc_enabled_already_count,\n",
    "    @cdc_disabled_already_count cdc_disabled_already_count,\n",
    "    @ct_enabled_already_count ct_enabled_already_count,\n",
    "    @ct_disabled_already_count ct_disabled_already_count;\n",
    "\"\"\"\n",
    "\n",
    "cdc_cd_tsql=jinja2.Template(\"\"\"\n",
    "-- CHANGE schema_name to your schema name\n",
    "BEGIN\n",
    "DECLARE @mode NVARCHAR(10) = N'{{mode}}'; -- CDC | CT | BOTH\n",
    "DECLARE @schema_name nvarchar(128) = N'{{source_schema_name}}';\n",
    "DECLARE @TABLE_CAT nvarchar(128), @TABLE_SCHEM nvarchar(128), @TABLE_NAME nvarchar(128), @PK nvarchar(128), @CT nvarchar(128), @CDC nvarchar(128);\n",
    "\n",
    "-- set if cdc or ct is enabled on the catalog\n",
    "DECLARE @ct_enabled_on_cat INT;\n",
    "DECLARE @cdc_enabled_on_cat INT;\n",
    "if exists(select is_cdc_enabled from sys.databases where name=db_name() and is_cdc_enabled=1)\n",
    "    set @cdc_enabled_on_cat = 1;\n",
    "else\n",
    "    set @cdc_enabled_on_cat = 0;\n",
    "\n",
    "if exists(select database_id from sys.change_tracking_databases where database_id=db_id())\n",
    "    set @ct_enabled_on_cat = 1;\n",
    "else\n",
    "    set @ct_enabled_on_cat = 0;\n",
    "\n",
    "{% if run_cdc_ct %}\n",
    "DECLARE MyCursor CURSOR FOR\n",
    "{% endif %}\n",
    "with \n",
    "tab as (\n",
    "\tselect table_catalog TABLE_CAT, table_schema TABLE_SCHEM, table_name TABLE_NAME \n",
    "\tfrom INFORMATION_SCHEMA.TABLES \n",
    "\twhere table_type='BASE TABLE'\n",
    "\tand table_name not in ('MSchange_tracking_history', 'systranschemas')\n",
    "\t{{s_t_filter_sql}}\n",
    "\t)\n",
    ", pk as (\n",
    "\t-- PRIMARY KEY TABLES\n",
    "    SELECT \n",
    "        tc.constraint_catalog as TABLE_CAT, tc.constraint_schema as TABLE_SCHEM, tc.table_name as TABLE_NAME \n",
    "    FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc \n",
    "    JOIN INFORMATION_SCHEMA.CONSTRAINT_COLUMN_USAGE AS ccu \n",
    "        on tc.constraint_schema = ccu.constraint_schema and tc.constraint_name = ccu.constraint_name \n",
    "    JOIN INFORMATION_SCHEMA.COLUMNS AS c \n",
    "        ON c.table_schema = tc.constraint_schema AND tc.table_name = c.table_name AND ccu.column_name = c.column_name\n",
    "    where tc.constraint_type='PRIMARY KEY'\n",
    "    and tc.constraint_schema = @schema_name\n",
    "    )\n",
    ", ct as (    \n",
    "    -- CT enabled tables\n",
    "    select db_name() TABLE_CAT, schema_name(t.schema_id) TABLE_SCHEM, t.name TABLE_NAME  \n",
    "    from sys.change_tracking_tables ctt \n",
    "    left join sys.tables t on ctt.object_id = t.object_id\n",
    "    where t.schema_id=schema_id(@schema_name)\n",
    ")\n",
    ", cdc as (\n",
    "    -- CDC enabled table\n",
    "    select db_name() TABLE_CAT, s.name TABLE_SCHEM, t.name as TABLE_NAME \n",
    "    from sys.tables t\n",
    "    left join sys.schemas s on t.schema_id = s.schema_id\n",
    "    where t.is_tracked_by_cdc=1 and \n",
    "    t.schema_id=schema_id(@schema_name)\n",
    ")\n",
    "select tab.TABLE_CAT, tab.TABLE_SCHEM, tab.TABLE_NAME, pk.TABLE_NAME PK, cdc.TABLE_NAME CDC, ct.TABLE_NAME CT \n",
    "from tab\n",
    "left join pk  on pk.TABLE_CAT=tab.TABLE_CAT  and pk.TABLE_SCHEM=tab.TABLE_SCHEM  and pk.TABLE_NAME=tab.TABLE_NAME\n",
    "left join ct  on ct.TABLE_CAT=tab.TABLE_CAT  and ct.TABLE_SCHEM=tab.TABLE_SCHEM  and ct.TABLE_NAME=tab.TABLE_NAME\n",
    "left join cdc on cdc.TABLE_CAT=tab.TABLE_CAT and cdc.TABLE_SCHEM=tab.TABLE_SCHEM and cdc.TABLE_NAME=tab.TABLE_NAME\n",
    "{% if run_cdc_ct %}\n",
    "{{cdc_ct_enable_tsql}}\n",
    "{% endif %}\n",
    "END\n",
    "\"\"\")\n",
    "\n",
    "sql_cmd_status = sa.text(cdc_cd_tsql.render(mode=mode, source_schema_name=source_schema_name, cdc_ct_enable_tsql=cdc_ct_enable_tsql, s_t_filter_sql=s_t_filter_sql, run_cdc_ct=False))\n",
    "sql_cmd_alter  = sa.text(cdc_cd_tsql.render(mode=mode, source_schema_name=source_schema_name, cdc_ct_enable_tsql=cdc_ct_enable_tsql, s_t_filter_sql=s_t_filter_sql, run_cdc_ct=True))\n",
    "\n",
    "display(pd.read_sql(sql_cmd_status, conn))\n",
    "if run_cdc_ct:\n",
    "    display(pd.read_sql(sql_cmd_alter, conn))\n",
    "    display(pd.read_sql(sql_cmd_status, conn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run insert / update / delete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine.connect() gets a new cursor to execute in the background for 30 minutes\n",
    "# every sec=6 rows insert, 1 row delete, 1 row update\n",
    "# pd.read_sql(\"select max(pk) from robertlee.intpk\", conn) # to see the number of rows in the table \n",
    "scheduler.add_job(engine.connect().execute, id=\"ins upd del\", run_date=datetime.now() + timedelta(minutes=1),\n",
    "args=[sa.text(f\"\"\"\n",
    "BEGIN\n",
    "DECLARE @Counter INT\n",
    "SET @Counter=1\n",
    "while ( @Counter <= 1800 )\n",
    "begin\n",
    "IF OBJECT_ID(N'{DB_SCHEMA}.intpk', N'U') IS NOT NULL\n",
    "    begin\n",
    "    insert into [{DB_SCHEMA}].[intpk] (dt) values (CURRENT_TIMESTAMP),(CURRENT_TIMESTAMP), (CURRENT_TIMESTAMP)\n",
    "    delete from [{DB_SCHEMA}].[intpk] where pk=(select min(pk) from [{DB_SCHEMA}].[intpk])\n",
    "    update [{DB_SCHEMA}].[intpk] set dt=CURRENT_TIMESTAMP where pk=(select min(pk) from [{DB_SCHEMA}].[intpk])\n",
    "    end\n",
    "IF OBJECT_ID(N'{DB_SCHEMA}.dtix', N'U') IS NOT NULL\n",
    "    insert into [{DB_SCHEMA}].[dtix] (dt) values (CURRENT_TIMESTAMP),(CURRENT_TIMESTAMP),(CURRENT_TIMESTAMP)\n",
    "WAITFOR DELAY '00:00:01'\n",
    "SET @Counter  = @Counter  + 1\n",
    "END\n",
    "END\n",
    "\"\"\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stage and target schemas and schedule deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    w.schemas.get(f\"{stg_catalog_name}.{stg_schema_name}\")\n",
    "except:\n",
    "    w.schemas.create(f\"{stg_schema_name}\", catalog_name=f\"{stg_catalog_name}\")\n",
    "    scheduler.add_job(w.schemas.delete, id='delete staging', run_date=datetime.now() + timedelta(minutes=60), \n",
    "        args=[f\"{stg_catalog_name}.{stg_schema_name}\"], kwargs={'force':True})\n",
    "\n",
    "try: \n",
    "    w.schemas.get(f\"{target_catalog_name}.{target_catalog_name}\")\n",
    "except:\n",
    "    w.schemas.create(f\"{target_schema_name}\", catalog_name=f\"{target_catalog_name}\")\n",
    "    scheduler.add_job(w.schemas.delete, id='delete target', run_date=datetime.now() + timedelta(minutes=60), \n",
    "        args=[f\"{target_catalog_name}.{target_schema_name}\"], kwargs={'force':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Gateway and Ingestion Pipelines and schedule deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4977185f-b5a9-4455-86a7-cec09337c908",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the Gateway pipeline"
    }
   },
   "outputs": [],
   "source": [
    "# determine the connection id\n",
    "connection_id = connections.connection_id\n",
    "\n",
    "gateway_def = pipelines.IngestionGatewayPipelineDefinition(\n",
    "      connection_id=connection_id,\n",
    "      gateway_storage_catalog=stg_catalog_name, \n",
    "      gateway_storage_schema=stg_schema_name,\n",
    "      gateway_storage_name = gateway_pipeline_name)\n",
    "\n",
    "p = w.pipelines.create(\n",
    "    name = gateway_pipeline_name, \n",
    "    gateway_definition=gateway_def, \n",
    "    notifications=notifications,\n",
    "    development=True,  # for faster restart\n",
    "    clusters= [ gateway_cluster_spec.as_dict() ] if None != gateway_cluster_spec else None\n",
    "    )\n",
    "gateway_pipeline_id = p.pipeline_id\n",
    "\n",
    "scheduler.add_job(w.pipelines.stop, id='stop gateway', run_date=datetime.now() + timedelta(minutes=30), args=[gateway_pipeline_id])\n",
    "scheduler.add_job(w.pipelines.delete, id='delete gateway', run_date=datetime.now() + timedelta(minutes=60), args=[gateway_pipeline_id])\n",
    "\n",
    "print(f\"Gateway pipeline {gateway_pipeline_name} created: {gateway_pipeline_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40066b15-0942-4031-9c53-fa0b8b098880",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create the Ingestion Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "if hasattr(pipelines, 'IngestionPipelineDefinition'):\n",
    "    ingestion_def = pipelines.IngestionPipelineDefinition(\n",
    "        ingestion_gateway_id=gateway_pipeline_id,\n",
    "        objects=tables_to_replicate,\n",
    "        )\n",
    "else:\n",
    "    ingestion_def = pipelines.ManagedIngestionPipelineDefinition(\n",
    "        ingestion_gateway_id=gateway_pipeline_id,\n",
    "        objects=tables_to_replicate,\n",
    "        )\n",
    "    \n",
    "p = w.pipelines.create(\n",
    "    name = ingestion_pipeline_name, \n",
    "    ingestion_definition=ingestion_def, \n",
    "    notifications=notifications,\n",
    "    serverless=True,\n",
    "    photon=True,\n",
    "    continuous=True,\n",
    "    development=True,   # for faster restart \n",
    "    )\n",
    "ingestion_pipeline_id = p.pipeline_id\n",
    "\n",
    "scheduler.add_job(w.pipelines.stop, id='stop ingestion', run_date=datetime.now() + timedelta(minutes=30), args=[ingestion_pipeline_id])\n",
    "scheduler.add_job(w.pipelines.delete, id='delete ingestion', run_date=datetime.now() + timedelta(minutes=60), args=[ingestion_pipeline_id])\n",
    "\n",
    "print(f\"Ingestion pipeline {ingestion_pipeline_name} created: {ingestion_pipeline_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schedule source tables delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.add_job(conn.execute, id='drop tables', run_date=datetime.now() + timedelta(minutes=30), args=[\n",
    "sa.text(f\"\"\"\n",
    "BEGIN\n",
    "IF EXISTS (select db_name() TABLE_CAT, schema_name(t.schema_id) TABLE_SCHEM, t.name TABLE_NAME  \n",
    "from sys.change_tracking_tables ctt left join sys.tables t on ctt.object_id = t.object_id \n",
    "where t.schema_id=schema_id('{DB_SCHEMA}'))\n",
    "BEGIN\n",
    "    ALTER TABLE [{DB_SCHEMA}].[intpk] DISABLE CHANGE_TRACKING \n",
    "    select 'disabled'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    select 'already disabled'\n",
    "END\n",
    "\n",
    "IF EXISTS (select db_name() TABLE_CAT, s.name TABLE_SCHEM, t.name as TABLE_NAME \n",
    "from sys.tables t left join sys.schemas s on t.schema_id = s.schema_id \n",
    "where t.is_tracked_by_cdc=1 and t.schema_id=schema_id('{DB_SCHEMA}'))\n",
    "BEGIN\n",
    "    EXEC sys.sp_cdc_disable_table @source_schema = N'{DB_SCHEMA}', @source_name = N'dtix', @capture_instance = N'all'\n",
    "    select 'disabled'\n",
    "END\n",
    "ELSE\n",
    "BEGIN\n",
    "    select 'already disabled'\n",
    "END\n",
    "\n",
    "IF EXISTS (SELECT 1 FROM sys.schemas WHERE name = '{DB_SCHEMA}')\n",
    "BEGIN\n",
    "    IF OBJECT_ID(N'[{DB_SCHEMA}].[intpk]', N'U') is NOT NULL\n",
    "    BEGIN\n",
    "        EXEC('DROP table [{DB_SCHEMA}].[intpk]')\n",
    "    END\n",
    "    IF OBJECT_ID(N'[{DB_SCHEMA}].[dtix]', N'U') is NOT NULL\n",
    "    BEGIN\n",
    "        EXEC('DROP table [{DB_SCHEMA}].[dtix]')\n",
    "    END\n",
    "    -- EXEC('DROP SCHEMA [{DB_SCHEMA}]')\n",
    "    -- leave the schema\n",
    "    select 'dropped'\n",
    "END\n",
    "else\n",
    "BEGIN\n",
    "    select 'already dropped'\n",
    "END\n",
    "END\n",
    "\"\"\")])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show delete job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in scheduler.get_jobs(): \n",
    "    print(job.id, job.next_run_time, job.name, job.args, job.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for pipelines to be online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_pipeline(pipeline_id):\n",
    "    found_running_completed_state=False\n",
    "    while 1:\n",
    "        pipeline_status=w.pipelines.get(pipeline_id)\n",
    "        print(f\"{pipeline_status.state=} {pipeline_status.latest_updates[0].state=}\")\n",
    "        found_running_completed_state = False\n",
    "        for state in pipeline_status.latest_updates:\n",
    "            if state.state.name in ('IDLE', 'RUNNING', 'CANCELED'):\n",
    "                found_running_completed_state = True\n",
    "                break\n",
    "        if found_running_completed_state:\n",
    "            break\n",
    "        print(conn.execute(sa.text(f\"select '{DB_SCHEMA}.intpk', max(pk) from [{DB_SCHEMA}].intpk\")).fetchall(), end='\\r') # to see the number of rows in the table\n",
    "        time.sleep(60)\n",
    "\n",
    "wait_pipeline(gateway_pipeline_id)\n",
    "wait_pipeline(ingestion_pipeline_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show row count of intpk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    print(conn.execute(sa.text(f\"select '{DB_SCHEMA}.intpk', max(pk) from [{DB_SCHEMA}].intpk\")).fetchall(), end='\\r') # to see the number of rows in the table\n",
    "    time.sleep(1)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update gateway pipeline to a bigger node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gateway_update_to_larger_node():\n",
    "    gateway_cluster_spec = pipelines.PipelineCluster(\n",
    "                label=\"default\",\n",
    "                driver_node_type_id=\"r5.2xlarge\",\n",
    "            )\n",
    "\n",
    "    # reenter the original specs with clusters as the only change\n",
    "    w.pipelines.update(\n",
    "        pipeline_id=gateway_pipeline_id,    \n",
    "        name = gateway_pipeline_name, \n",
    "        gateway_definition=gateway_def, \n",
    "        notifications=notifications,\n",
    "        development=True,  # for faster restart\n",
    "        clusters=[ gateway_cluster_spec ] if None != gateway_cluster_spec else None\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "sql-server-cdc-connector-setup (1)",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
